today we're going to talk about the branch and bound method this is a well-known and popular method for discrete optimization it can be a kind of a general purpose method although it's more effective for certain classes of problems we'll talk about that today but for general problems it's not necessarily very efficient if there's other structure in the problem there are often better methods but for certain types of problems as we'll see this is a very effective strategy um so uh at the heart of the method we'll see one of the things that we do is sort of this bound we're going to keep track of what a lower bound is or in other words the best solution uh or the best the solution could possibly be and to to know that we actually have the bound means that we have to be able to guarantee that we can solve each sub-problem and find the global optimum there's only certain types of problems that we can guarantee that we'll find the global optimum these are called convex problems we haven't really discussed this although we have discussed certain types of convex problems we talked about quadratic programming problems those form the heart of the sqp where say for example if there's equality constraints that reduce even to a linear system but for inequality we could also guarantee convergence as long as know that quadratic term was uh positive definite perhaps the most popular uh convex form for for these uh branch and bound problems Linear Programming is a linear problem so a linear problem looks like this this is just a general form we could say let's minimize uh some coefficient of vectors times x so c is just a vector of numbers right so these would just be some weights and so that's a linear objective right because this would be like c1 x1 plus c2 x2 plus c3 x3 and so on so you can see that's just linear in the variable x and all our constraints must also be linear so we could have for example ax plus b equals 0 some equality constraints for example we could also have and just to use a different symbol put a hat here ax plus b-hat is less than or equal to zero for inequality constraints so b here is a vector a is a matrix so each row of this is just a linear constraint right it would be something like say a 1 1 x 1 plus a 1 2 x 2 plus a 1 3 x 3 plus dot dot dot plus b 1 equals 0 for example and we can see there's just another linear equation and now we put in matrix form because there's a bunch of them okay so this here um we call it an lp or a linear programming problem this is convex meaning we are guaranteed to be able to find the global optimum we can solve these very efficiently and they're very popular and say operations research like um transportation problems networking problems uh all sorts of cases like this uh we did we did an in-class exercise just to to recap one example where um you actually did this in excel and that is using a branch and bound method um often when you solve these uh linear problems with a with discrete variables they'll use a branch and bound method but um in that example say this was like a serious cost right we're trying to minimize some cost and so these were the products we were making and we're making some integer number of them we're just summing up the cost and these linear constraints would be things like resources each widget i make takes a certain amount of resources and i can't exceed some you know specified number of resources so those are often linear constraints i want to figure out like the right mix of products to manufacture so these kind of problems occur a lot like in in sort of a manufacturing sense or in a transportation sense or maybe i want to minimize the distance or the cost of uh transportation we talked about kind of this um traveling salesman like problem or say i wanted to do a a uh this airport problem right where i was trying to go from airport to airport find a minimum cost maybe i have um you know various constraints on on on my shipping uh that i can pose in a linear form maybe there's some different uh weights or things that i can carry and i can't exceed a certain capacity so these occur quite frequently in in what what is called operations research um we can use them for other problems but we'll kind of recap that at the end they just in general are less effective because one of the things that we're going to rely on is that we can solve a sub problem or in other words a sub problem is going to be this problem we ignore for the moment the discrete part of it we need to make sure that we can guarantee a solution and then it's a global optimum and that's just not possible for general problems but we can here for these linear programming problems so this isn't really our complete problem that in itself was the lp but then we're going to add this additional constraint and i'm just going to uh i guess i can write it this way this is the mathematic notation here for all or some of i this just means this is this z said this is a set of integers and the positive means it's a positive integer so this means that all of my x's must be integers this is that integer constraint and here we're restricting to be positive integers that's the most common case here right where we're talking about how many goods to produce or um you know what uh nodes to connect or whatever uh it doesn't make sense to produce a negative amount of of some good generally um although it could in that case we could just without any loss of generality just put a negative weight here so this is actually completely general so i guess you can ignore what i said about negatives that's that's totally acceptable here um yeah and just to be clear that negative is captured here in the weight not not here so this is general that we assume that these x's are positive okay so these are positive integers and so without this it's an easy problem it's an lp but now that we've added this constraint that some of these variables um are discrete they're integers then it becomes a bit harder and we would call this a safe works most commonly um a mixed integer linear programming problem so this is a mi lp mixed integer mixed meaning there's potentially some integer some continuous variables okay so before we jump into the algorithm Converting to Binary one step that's very helpful is uh to convert to binary if possible so we just said the general problem is that these x's these design variables could be integer variables like one two three four five but it is much easier to solve usually the branch and bound problems if our variables are binary zero or one it is often though not always it's often possible to convert an integer problem into an equivalent binary problem sometimes we have to add maybe additional variables or constraints or whatever but we can often do that conversion so here's an example that you can think of so let's say we talked about some examples of integer problems actually mentioned this as discrete like i wanted to choose the material for my car you know should i make this out titanium aluminum steel or whatever different alloys of steel i could have a bunch of them so here's orig let's say this is my original problem i've got one design variable and this is uh we'll i'll give it a y so it's different from the x's and y can be 1 2 all the way up to n and these are represent n different materials okay so that would be the way we could specify the problem right we just map each material to a number one is steel two is aluminum and so on and the optimizer needs to choose that design variable that fits this form fine but to be more efficient we would like it to be a binary problem so uh you might pause at this moment pause the video and try that see if you can convert to a binary problem so here's something we could do to convert right so let's do some conversion here we could say well let's make more design variables instead of having one design variable we're going to have n design variables okay and each of these variables we want them to be binary so it's either zero or one and so zero means don't select that material boy have a hard time writing don't select and one means we do select so now i have n design variables not one but n one for every material and so it's either going to be a zero or a one i either select that material or don't obviously that's not good enough because i need to select just one material at the end so we're also going to add a constraint what kind of constraint could i add well i just need the sum of these to be equal to one right because i i need everything to be zero except for one material the one that i choose so i had the constraint that the sum of these is equal to one and that's a linear constraint so this all that's saying is that we only can select one material so we're able to successfully convert this problem this energy problem into a binary problem all my decision variables are now zeros or ones there are more of them and i added a constraint so it seems like it's harder but it's actually much easier usually if we do this even though it's a bigger problem it still ends up being easier to solve okay so we'll now go with the assumption that we've done that let's just look at an example here one that we're going to look throughout today here's just an example linear problem notice it has those binary constraints so here's the objective you can see it's a linear function in x and these are all linear constraints there's two constraints and then we have this binary constraint every x and here we don't have any continuous variables although we could in general but each one of them has to be a zero or one okay so um the key to solving these kind of problems or any branch and bound problem is we use what's called a relaxation and relaxation means we're going to solve an approximate problem where we're going to generally relax some of the constraints mean we're going to remove some of the constraints so the first way we're going to do this and this is how we'll do it throughout this is the natural way for at least these integer problems for other more general problems there might be other relaxations that could be better but in this case an easy relaxation is just to say let's get rid of this all right let's get rid of that constraint then it's an easy problem to solve uh every optimization package has linear programming solvers it's convex right so it's guaranteed itself global optimum very efficient very fast uh it's an easy problem to solve so that's great we can solve this problem so um and let's say just by chance when we solve this we happen to get all zeros and ones for the answers for x well then we'd be done right we solved the problem because we let it do whatever and it gave us theirs in one so we know that's actually is a solution is the solution to our original problem but that's unlikely to happen right so we have to uh keep moving forward however what it does tell us when we solve the problem it's not it's not a solution to the problem but every time we relax it we know that we couldn't do better than that that's a lower bound it's not a lower bound to solution yet but it's a lower bound meaning that's the best we could possibly do because if we add these constraints back in if i erase this it could give me the same answer if it did i would have been done already but in general adding more constraints is means i'm going to get a worse solution right worse in the sense of the objective not worse in the sense of our goal but a a a bigger objective what i'm trying to minimize so i know that uh that relaxed problem is a lower bound i can't i can't do better than that okay Branching so that's going to be an important point that we'll use uh here's maybe a visualization of a start on the idea of the branching portion of this so the branching portion is we first this is this first node here is this problem where we or we got rid of this so ignore that for the moment if we ignore that for the moment and we try and solve it and it didn't solve so now we're going to branch we're going to try two different options because x1 only has let's say we branch on x1 first x1 only has two options really it's only a zero or one in fact this problem is small enough right that we could try all the combinations but of course that is not scalable as we've seen but now i'm going to solve a new problem here where i choose x1 is 0 in this branch so this node here says let's try let's go back to this problem here let's solve it but we'll just keep x1 0. we won't optimize x1 we'll fix it at 0. solve for the other variables okay uh and we could keep going right we could go down further and we could say okay now let's keep x 1 0 and x 2 0 and try to solve for the other variables and maybe it gives me a solution maybe it doesn't uh and i could try keep going through all of these and you'll notice right if i i did all of these branches i would have done exhaustive search in other words i tried every combination and that's not good right because that kind of defeats the whole purpose of having an algorithm but what we'll see is that we can eliminate branches what's going to happen is that um uh you know we may meet some criteria say here that says we know nothing below here can be a solution so we can just get rid of this whole thing and not evaluate it or maybe we get here and we realize okay can't go any further here we can get rid of all of this this is called pruning when we cut off some of these branches okay so we've kind of had two key words here so far the branch part and we've mentioned briefly the bound part is the lower bound we're going to kind of put these together now so let's see how we could prune here's Pruning the diagram and this is really the key to this is that branching through every possibility is exhaustive search but if we can prune we can cut off branches we can severely restrict the number of combinations we have to find and still guarantee that we can find the optimum right if we have a linear problem this is one of the cases where we can find the solution right even though we didn't try every combination we're taking advantage of that convex structure all right so here are some some possible outcomes let's say i solve this problem that problem where i relax the problem right so in other words i got rid of the integer constraints and and that's going to repeat throughout right so i'm going to go through here and let's say i fixed x1 to be 0. i still eliminate those integer constraints and i try to solve it again and let's say when i try to solve it it's infeasible if that problem is infeasible what does that say about the rest of that branches if i went down further what it says is that that entire branch is infeasible so we can prune it because if i go down further in that tree remember all we're doing is we're adding more constraints so we're saying first just fix x1 to be 0. now fix x2 to be 0 right don't let it have anymore don't let it vary to whatever so if the problem could not be solved before fixing one of those variables and not letting it move is not going to let it be solved and easier right now it's it's still infeasible so once i find any solution that's infeasible let's let's say this one here is infeasible that means all of these are also infeasible and so i don't have to evaluate them right so in other words the general the general principle is if i can solve a problem like i and and really for this to work i have to guarantee that i have actually know that it's infeasible and this works in this case convex if i know for sure it's infeasible then adding more constraints can never change that right that won't ever make it any more feasible okay so we could prune that's great so another is that let's say the objective [Music] is worse than our best solution if the objective is worse than the best solution then what does it say about points that are further down on the branch remember every time we go down the branch we're adding more and more constraints well if we add more constraints and again this is true if we can make sure we're getting the global optimum adding more constraints will never improve the objective it might it may not make it worse but it can't make it better so we know that if we found a solution that's already not better than the best we found then nothing from that point on will be better so we could prune that as well so let's say we got to this point here we say oh this is we have a solution and here this isn't a solution even it may not even be a solution it's just it's the relaxed problem but we we know it's already not better than the best so adding more constraints even if there are solutions down there we know that they're worse than the best we've found so we can get rid of all of these and not evaluate that branch so if the objective is worse and the best solution we can prune and i guess the obvious implication there is that we do need to keep track of what our best solution is so we can compare Solution okay so those are those are the two pruning parts the two other possibilities is that we find a solution okay so maybe we find a solution and that means you know we get to some point here let's say i get to here and even though i didn't constrain everything you know i constrain x1 and x2 and x3 and x4 and whatever other variables happen to give me integers then that's a solution right i i satisfied that constraint of the binary zeros and ones so that's done so that's or the problem is not done because there may be many solutions here right enough to find the best but that means i can prune right in other words i save that solution i compare it to the best but there's no need going further right because again adding more constraints can't improve it so even if there were and there really can't be but if there are other solutions on that branch they can't be better so uh that's it right so this is a lot of things i can do to prune here otherwise we have to keep going that's really the fourth possibility is that we don't have a solution but it still looks promising it could be better than the best it's still feasible so we have to keep branching and go further down on that tree all right so again this is the point and i'll come back to this again these kind of points especially these two um actually really even the third is what hinges on the fact that we can find the global optimum for this relaxed problem this is why we need something like an lp a linear program problem or other convex problems because those we can guarantee it it's a general problem um and we're pruning because we say well it can't get any better that might be because it can't get better it might be just because we found a local optimum and you know we prune too early so we're not necessarily going to find the best solution it's uh and each of these sub problems becomes more complicated right because we have to solve a sequence of them so anyway we'll get back to that later this is the method let's kind of go through an example now well actually for example i'm going to talk about a few um sort of design choices here on the method so one question is um which variable to branch on so above figures i just branched on x1 first and then x2 just you know just for a picture but um let's say i solve a problem and there's only four variables and this is my relaxed problem i get uh just make something up here let's say i got one point two point six zero point one or whatever okay um two of these are already integers so i don't need to branch on them they might change later right they're still free to change uh but these ones i i should branch on because they're fractional so i'm going to take a branch i could do any one of these but a very a common choice is to branch on the one that is closest to 0.5 meaning it it's you know not close to zero or one the logic there is that these ones that are maybe closer to zero or closer to one they would say well they're they're maybe getting there right that probably could nudge that way more easily and here we're forcing a decision faster we're saying okay let's just force it try it with a zero try it with a one and resolve it so it tends to be more efficient if we if we use that component that's lar biggest fractional component i mean it's closest to 0.5 another design choice is should we do a depth first or a breadth breadth first okay that means when we branch should i continue down this branch all the way to the bottom until i can't go any further and then work my way up or should i go here and then here and then here and then here and here and here and you know back up as i need of course but what's the strategy and most commonly uh depth first is usually desirable and the reason is that by forcing more and more and more constraints we usually force a solution faster and that's helpful because once we have a solution we can compare it to others and that helps us prune faster usually it also means we don't need as much of a history because when we're doing depth first we just only have to keep track based on the number of nodes but if we do breadth first we have to the amount of things we have to keep track of and to keep in memory to compare grows as we get further and further down anyway so most commonly depth first although in examples today i'll actually show breadth first just because it's easier to show with these small examples and actually i think in the one case it went faster so i just uh we'll we'll do that today all right so here's that same problem we're looking at Example so let's step through an example here uh and work it um i actually won't solve all the sub problems but you can do that this example is in the book i'm just going to show you kind of the steps so what was the first step again first step was to relax the problem and i guess i was maybe being not specific enough i said we're going to eliminate these constraints which is true but we're not going to just eliminate them we're going to replace them with bound constraints so we're going to say we're making it a continuous problem but we are going to say that they should stay between zero and one right it makes no sense for us to just allow it to be whatever right it could give us two fifty whatever because we know at the end it either has to be a zero or one so let's keep it within those bounds at least it's still a continuous problem it's still a linear constraint easy to solve it's even a bound constraint so we can solve this still really easily so let's say we do that okay we solve it um and here's what we get here's the solution that linear problem and again you could plug this into any linear programming solver it'll be easy for the optimizer to deal with this and so we get 1 0.5 i'm just going to write two decimal places here to from brevity 0.53 zero point uh two decimal places that's five zero one okay so those are my variables um and the optimal f just for interest here is minus 5.03 just with two decimal places so this is not a solution right because i have two that are binary but these two are not so i have to branch i'm going to branch on the one that's closest to 0.5 which is this one on x3 okay so what does that mean so here's again here's that first solution we saw i'm branching on x3 so i'm going to solve this same problem the same problem except for i'm going to now take x3 out of it i'm going to say in this step let's just say x3 equals zero so it's not a design variable anymore and now i can i can let x1 x2 and x4 very freely right still within these bounds but they can vary freely okay and that's going to be this node the other node is same problem but force x3 to be one so x3 is forced to be one and then we let the others go freely okay so here's what happens here right x3 was forced to be zero and we let the other three vary freely and in this case uh it's still not a solution so we're going to continue and let's try this side right normally i said we do depth first but you know for this picture it's actually easier for me to show you breath first so here let's do this breath one and here we solve the problem remember we fixed x3 to be one we let the others vary notice right all the others are free to vary so even though x4 was one before it's not one now we didn't force it to be one right we're letting it be whatever it wants to be um and it's uh this is also not a solution so we have to keep going okay notice that each time we there's another thing notice each time we go down right this constraint or sorry this objective this is our lowest bound this is the best that could possibly occur now we know it's not a solution so the real solution is going to have a higher objective so as i go down and i've added more constraints notice each of these is worse they could be the same but they can't be better okay so now this is i know i can't do better than that at least on this branch here so let's let's keep going here actually this was pretty close right one zero one so the only one i could branch on is this x2 so let's branch on that okay so here we are i branched on x2 and so so now i still force x3 is forced to be zero and x2 is forced to be zero and i let one and four go free i let them be whatever within the bounds zero and one and they both hit the bounds that was the best solution could find with those constraints so this is a solution that's great so we're going to keep track of that we're now saying the best this is the best solution we've found so something we're going to compare to f star is minus 4. it doesn't mean there are another solution we don't know that we've actually solved it we just know that anything any number that is um already bigger than that well we know that this is a lower bound we can't anything that's a solution or sorry any any value that's higher than that is not going to be useful okay so let's keep going um i'm just going to say something on that i don't know i'm not sure i said that right let me get back to that in a sec here so let's do this next i'll recap that here at the end so let me just finish the rest of this row here okay so i solved the next one because again i still have this branch i wasn't able to prune it yet uh here it is right so we fixed x3 at zero we fixed x2 at one let the other two vary uh we get a solution it's not a solution right because this is still not binary so we would in general have to continue here this one gave us a solution this is uh all binary values so this is a solution and this is now our new best minus 4.9 is the best and then this one is not a solution but here's the thing that we notice is that now we have updated our best it's minus 4.9 so anything that is worse than that um yeah i know what i misspoke on let me correct that in a second um anything that is worse than that uh yeah sorry i'm thinking of two things at once okay so anything anyway this is a solution so this this means that this value here we know as we go further we're adding more constraints so this objective cannot improve further right anything that's below here is going to be bigger than minus 4.49 but we already know that we have a solution that's minus 4.9 so there is no need to continue this branch any further right anything below there is either going to be minus 4.49 or worse right bigger same with this branch here right we got minus 4.52 so there's nothing uh you know if we keep going we could branch further but we know even if there's a solution there it's not going to be better than this one right because minus 4.52 is already worse um yeah so okay yeah i i i did speak correctly right so the idea here is that this was minus four let's say that this value here was um minus three it was already a worse solution we wouldn't even have had to evaluate those but because it's minus five it's not a solution but we know that there might be a solution that's still really good right because this is giving us minus five but as we come down and now we got new value minus four point nine anything that's bigger than that which is this and this we can prune so now we're done we've pruned every branch there's nowhere else to go so this is the answer this is the optimum the only solution to this or yeah not the only solution i guess the solution this is the global optimum to our discrete problem and we're able to do that again because of this linear structure that we're taking advantage of that we know we can solve those sub problems globally okay so again an important point to note here is that even though there was a solution we're not done we have to make sure we've exhausted uh each or pruned every branch and again this is not an exhaustive search here because we only went two levels x3 and x2 there could have been two more and of course in general there could be lots right depending on the variables so that was breadth first i'm not going to walk through it entirely but i just want to show you what it looks like with depth first in this case so uh here the the depth first was just always to go left first so we branched on x3 like before um that was not a solution so we branched again we found this solution exactly like we did before so then we come back up we come here it's not a solution so we branch we found a new solution but it's worse it's worse than this one right so keep going branched another solution it's still not as good branch and then this is infeasible we've now constrained everything and it's actually not a solution so we back all the way up come down and here we get a solution this is a our new best minus 4.9 so we save it and then we come here and it's bounded just like it is here right this is it gives us minus 4.49 so we know well that's worse than the best we've found so far so there's no need to continue in this case the breadth first happened to be faster just because the solution was here but in general depth first is usually a preferred strategy okay so what if you couldn't use binary we talked about this whole thing with binary and as we said you can often convert integer problems to binary but if you couldn't another thing you can do is you could do the same kind of idea so let's say we're required to have integers right um and uh let's say um you know i've got 1 3 6.27 let's say these are my variables and i now want to branch on this one i can't make it 0 and 1 but i could do this i could say let's do one branch where x3 has to be less than or equal to 6 and then the other where x3 has to be greater than or equal to 7. so we're trying to put those integers on those bounds and because it wanted to be in between those two it's likely that it's going to hit one of those bounds not necessarily though this tends to well this is in general not as efficient i'll just show you as a picture i'm not going to walk through in real detail but this is in the book here's another problem another linear problem you can see and here this one is mixed so the first three variables one two and three have to be integers not binary just any integer and x4 is continuous just any continuous variable you know positive variable so here's uh what this looks like so we branch for example on x3 and based on what its value was we said it had to be less than equal to four greater than equal to five and then we keep doing that um you'll see though that uh one of the problems of this method is that in the binary case as you branch you would never visit a variable more than once because every variable only has one possibility or two possibilities right it's either zero or one so once we branched on x3 you would never see x3 appear again in another branch but here it does right so x3 is less than equal to four but then we also see well maybe x3 is less than or equal to two we're tightening those constraints um x3 is greater than equal three right so uh just because we added one constraint we may have to add other constraints to kind of tighten that window and so these branches can become quite a bit longer sometimes whereas they're bounded for the the binary case that's one reason why we uh the binary is advantageous okay so as a final point here just to recap we did everything in the context of linear or really convex problems in general but you can do this for a general problem and uh there are branch and bound methods for that we basically use a similar type of idea where we use these kind of constraints we'll take these continuous constraints and try to branch on constraints the problem in that case is that this is not a guaranteed method it's just a heuristic that we hope it will work well uh but because we're not guaranteed to find the global optimus step a lot of those uh pruning steps can be overly optimistic and and you know not give us a good solution the sub problems because we solve a whole bunch of problems if they're not convex and easily solvable that can become uh expensive you know especially because these trees can be large for large for large numbers of variables so if you don't have a linear problem it's usually only practical with a smaller number of variables otherwise the you know the tree it gets too complicated but anyway uh that's branch and bound this is like i said this is a popular method so if you are solving a mixed integer linear program it's likely that the algorithm you're using is a branch and bound inbound it's like i said particularly effective for these linear problems or or other convex problems okay so uh next time we'll continue our discussion with discrete optimization see you uh see then